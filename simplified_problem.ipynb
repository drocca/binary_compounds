{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge on stable compounds: A simplified multilabel symmetric problem\n",
    "__Author: Dario Rocca__\n",
    "\n",
    "The labels to be predicted are given in the form [1.0,0.0,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.0,1.0] corresponding to pairs of elements formulaA and formulaB. Specifically, the labels correspond to the 1D binary phase diagram: [100% element A, (90% element A-10% element B), (80% A-20% B), (70% A-30% B), (60% A-40% B), (50% A-50% B), (40% A-60% B), (30% A-70% B), (20% A-80% B), (10% A-90% B), 100% B] where a 1 indicates stability and a 0 indicates unstability of the corresponding compound. This can be seen as a multilabel problem. As all the elements are stable in their pure form we will drop 100% A and 100% B (first and last label). The specific list depends on the order of the elements. For example, the pair Ac-Tl corresponds to [1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0] while Tl-Ac would correspond to [1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0], which is the inverse of the previous diagram. I found difficult for machine learning algorithms (random forest, neural network, one-vs-rest SVC, etc.) to learn this property of the data. <br>\n",
    "\n",
    "In this notebook I will start by simplifying the problem. Specifically, I will \"symmetrize\" the labels to create a list (or array) with only 5 entries [(90% A-10% B) or (10% A-90% B), (80% A-20% B) or (20% A-80% B), (70% A-30% B) or (30% A-70% B), (60% A-40% B) or (40% A-60% B), (50% A-50% B)]. For example, within this formulation we transform [1.0,0.0,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.0,1.0] into [1.0,1.0,1.0,1.0,0.0]. These labels cannot be used for the final prediction as they contain less information: For example, if a 1 occurs in the first position it is not anymore possible to distinguish if the stable mixture is (90% A-10% B), (10% A-90% B), or both. These labels are not affected by the problem of the element order and element swapping as formulaA-formulaB and formulaB-formulaA correspond exactly to the same labels. This simplified problem is meant to let us estimate the level of subset accuracy (https://en.wikipedia.org/wiki/Multi-label_classification#Statistics_and_evaluation_metrics)\n",
    "that can be achieved by using the features provided by the challenge and by considering the problem as multilabel. Here I will use random forest which is a quite powerful algorithm and can be trained for multilabel problems. When I will go back to the full problem which has more degrees of freedom (more labels), it will be challenging to improve over the \"accuracy\" of the simplified problem of this notebook.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the training set with Pandas\n",
    "\n",
    "train=pd.read_csv(\"training_data.csv\") # creating the train dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of \"symmetric\" labels to be predicted\n",
    "\n",
    "As explained in the introduction I will transform the full set of labels [100% element A, (90% element A-10% element B), (80% A-20% B), (70% A-30% B), (60% A-40% B), (50% A-50% B), (40% A-60% B), (30% A-70% B), (20% A-80% B), (10% A-90% B), 100% B] into a \"symmetric\" set [(90% A-10% B) or (10% A-90% B), (80% A-20% B) or (20% A-80% B), (70% A-30% B) or (30% A-70% B), (60% A-40% B) or (40% A-60% B), (50% A-50% B)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The labels to predict are strings of the form \"[1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0]\"\n",
    "# Below I will extract the corresponding list\n",
    "\n",
    "import ast\n",
    "\n",
    "list_label = train['stabilityVec'].tolist()    # A list that contains the column 'stabilityVec'\n",
    "list_label = map(ast.literal_eval,list_label)\n",
    "list_label_np = np.asarray(list_label)         # Now we have an array containing all the 1D arrays of the labels\n",
    "\n",
    "\n",
    "#Let's drop the stability index of the pure element, as we do not need to predict it\n",
    "#and it might bias the model\n",
    "list_label_compound = list_label_np[:,1:10]\n",
    "\n",
    "#Creating the numpy array that will contain the simplified symmetric labels\n",
    "list_label_compound_sym = np.zeros((2572, 5), dtype='float32')\n",
    "\n",
    "#Creating the simplified symmetrized labels (see introduction)\n",
    "for i in range(len(list_label_compound)):\n",
    "    list_label_compound_sym[i][4]=list_label_compound[i][4]\n",
    "    if (list_label_compound[i][0]==1 or list_label_compound[i][8]==1):\n",
    "        list_label_compound_sym[i][0]=1.0\n",
    "    if (list_label_compound[i][1]==1 or list_label_compound[i][7]==1):\n",
    "        list_label_compound_sym[i][1]=1.0\n",
    "    if (list_label_compound[i][2]==1 or list_label_compound[i][6]==1):\n",
    "        list_label_compound_sym[i][2]=1.0\n",
    "    if (list_label_compound[i][3]==1 or list_label_compound[i][5]==1):\n",
    "        list_label_compound_sym[i][3]=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## I'm dropping the column of labels 'stabilityVec'\n",
    "## I'm dropping also the element name columns 'formulaA' and 'formulaB'; previously I tried\n",
    "## to include their one-hot encoding: It did not help the model and adds several features\n",
    "\n",
    "X_tot = train.drop(['formulaA','formulaB','stabilityVec'], axis=1) \n",
    "Y_tot = list_label_compound_sym # Symmetrized target values\n",
    "\n",
    "\n",
    "# Holding out 10% of the data to test the model \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_tot, Y_tot, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a multilabel model and testing it\n",
    "\n",
    "In the next cell I will train a random forest multilabel model and I will use it to predict the labels. I used 10-fold cross validation to perform a grid search to find the optimal parameters for the model. As grid search with crossvalidation takes time, the corresponding lines of code have been commented.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the training set.\n",
      "\n",
      "These scores are computed on the training set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98        98\n",
      "          1       1.00      1.00      1.00       421\n",
      "          2       1.00      1.00      1.00       753\n",
      "          3       1.00      0.99      0.99       290\n",
      "          4       1.00      1.00      1.00       522\n",
      "\n",
      "avg / total       1.00      1.00      1.00      2084\n",
      "\n",
      "\n",
      "Subset accuracy 0.996110630942\n",
      "\n",
      "These scores are computed on the hold out test set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.17      0.27        12\n",
      "          1       0.79      0.48      0.60        48\n",
      "          2       0.82      0.76      0.79        83\n",
      "          3       0.70      0.45      0.55        31\n",
      "          4       0.79      0.68      0.73        62\n",
      "\n",
      "avg / total       0.78      0.61      0.68       236\n",
      "\n",
      "\n",
      "Subset accuracy 0.670542635659\n"
     ]
    }
   ],
   "source": [
    "# Importing sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import coverage_error \n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "################################################################################\n",
    "\n",
    "####### The following commented lines have been used to optimize the RandomForestClassifier parameters\n",
    "####### This takes some time \n",
    "\n",
    "## Important parameters to optimize: n_estimators, max_features, and max_depth\n",
    "# search_grid_rf = [{'n_estimators': [10, 20, 40, 80, 100, 150, 200, 300, 400],\n",
    "#                 'max_features': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7],\n",
    "#                 'max_depth': [2, 4, 8, 12, 14, 16, 18, 24, 30]}] \n",
    "\n",
    "# I'm using subset accuracy as score function\n",
    "# subsetacc = make_scorer(accuracy_score)\n",
    "# multilabel_rf = GridSearchCV(RandomForestClassifier(random_state=0), search_grid_rf, cv=10,\n",
    "#                        scoring=subsetacc)\n",
    "\n",
    "# multilabel_rf.fit(X_train, Y_train)\n",
    "\n",
    "# print \"Best parameters set found on traning set:\"\n",
    "# print \"\"\n",
    "# print multilabel_rf.best_params_\n",
    "# print \"\"\n",
    "# print \"Grid scores on training set:\"\n",
    "# print \"\"\n",
    "\n",
    "# means = multilabel_rf.cv_results_['mean_test_score']\n",
    "# stds = multilabel_rf.cv_results_['std_test_score']\n",
    "# for mean, std, params in zip(means, stds, multilabel_rf.cv_results_['params']):\n",
    "#    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "#            % (mean, std * 2, params))\n",
    "# print \"\"\n",
    "\n",
    "## Best parameters set found on traning set:\n",
    "\n",
    "## {'max_features': 0.1, 'n_estimators': 100, 'max_depth': 16}\n",
    "\n",
    "## Grid scores on training set:\n",
    "\n",
    "## 0.642 (+/-0.057) for {'max_features': 0.1, 'n_estimators': 100, 'max_depth': 16}\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# The multilabel classifier\n",
    "multilabel_rf = RandomForestClassifier(max_features = 0.1, n_estimators = 100, max_depth = 16, random_state=0)\n",
    "# With the parameters above I got 0.642 (+/-0.057) as best score (subset accuracy) in the grid search\n",
    "# random_state=0 allows for reproducibility; alternatively we could release this constraint \n",
    "# and average over different final results\n",
    "\n",
    "###########################\n",
    "\n",
    "multilabel_rf.fit(X_train, Y_train)\n",
    "predictions_train = multilabel_rf.predict(X_train)\n",
    "\n",
    "print \"Detailed classification report:\"\n",
    "print \"\"\n",
    "print \"The model is trained on the training set.\"\n",
    "print \"\"\n",
    "print(\"These scores are computed on the training set.\")\n",
    "print \"\"\n",
    "print(classification_report(Y_train, predictions_train))\n",
    "print \"\"\n",
    "\n",
    "print \"Subset accuracy\", accuracy_score(Y_train, predictions_train)\n",
    "\n",
    "\n",
    "###########################\n",
    "\n",
    "predictions_test = multilabel_rf.predict(X_test)\n",
    "\n",
    "\n",
    "print \"\"\n",
    "print(\"These scores are computed on the hold out test set.\")\n",
    "print \"\"\n",
    "print(classification_report(Y_test, predictions_test))\n",
    "print \"\"\n",
    "\n",
    "print \"Subset accuracy\", accuracy_score(Y_test, predictions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "- I trained the parameters of the model with 10-fold cross-validation and using subset accuracy as scoring function. __On the test set I obtained a 67% subset accuracy.__ As a reference, by considering all the labels as equal to 0 the subset accuracy would be 52%. Subset accuracy is a very strict metric as it indicates the percentage of samples that have all their labels classified correctly. As shown above certain classes have sizeably lower f1-score and this has implications for the overall subset accuracy.  \n",
    "- It can be noticed that the class 2 has the highest f1-score followed by class 4. Vice versa the classes 0 and 3 have the lowest f1-scores. This can be easily understood by the fact that the classes 2 (30%-70% and vice versa) and 4 (50%-50%) have the largest number of training examples while the classes 0 (10%-90% and vice versa) and 3 (40%-60% and vice versa) are more rare. I also verified that, even if I uded a random shuffle, there are no problems with the stratification of the data and all the 5 labels are represented in a balanced way between training and test set.\n",
    "- Precision is sizeably larger than recall. This means in practice that when my model labels an compound as stable, the prediction is \"likely\" to be correct; on the other side the model still misses some stable compounds. It would be possible to change the precision-recall balance by changing the threshold used for the prediction (0.5 by default).\n",
    "- It is not shown in the notebook but I tried to introduce new features as weighted averages of the properties of the elements. Specifically, I introduced features of the type 0.9\\*featureA+0.1\\*featureB, 0.1\\*featureA+0.9\\*featureB, 0.8\\*featureA+0.2\\*featureB, etc. I considered several choices for the specific featureA and featureB. I noticed that the derived average features were helping to improve the accuracy of certain labels and deteriorating the accuracy of others. As a matter of fact in a multilabel problem the algorithm might not understand that a certain feature is meant to be used only to improve the prediction of a specific label. For example, the derived feature 0.9\\*featureA+0.1\\*featureB can influence not only the mixture 90%A-10%B but also the mixture 50%A-50%B. To solve this problem I thought about training a different model with different features for each different label. I understood rapidly that this was not a suitable idea as I would have lost the correlation between labels and I would have risked to create a series of models completely disconnected. In order to exploit compound specific features, such as the weighted averages, I developed the idea presented in the notebook main.ipynb. Soon after I realized that this approach was the \"standard\" in the scientific literature in the field.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
