{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge on stable compounds: Binary classifier and compound dependent feature engineering\n",
    "__Author: Dario Rocca__\n",
    "\n",
    "The labels to be predicted are given in the form [1.0,0.0,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.0,1.0] corresponding to pairs of elements formulaA and formulaB. Specifically, the labels correspond to the 1D binary phase diagram: [100% A, (90% A-10% B), (80% A-20% B), (70% A-30% B), (60% A-40% B), (50% A-50% B), (40% A-60% B), (30% A-70% B), (20% A-80% B), (10% A-90% B), 100% B] where a 1 indicates stability and a 0 indicates instability of the corresponding compound.\n",
    "In the following the first and last labels will be dropped, as they correspond to the pure elements.\n",
    "\n",
    "While the problem described above can be straightforwardly adressed by a multilabel classifier, in this notebook I will show how it can be reformulated as a binary classifier. Specifically, we can unroll the vector of labels (for example [0.0,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.0]) into a column; now each row, namely each training example, has a single label that can take values 0 or 1. In this way I will get a set of training data that is much larger than the original one, with 2572*9 (n of original training data*n of labels) training examples. Also within this reformulation the problem can be addressed as a binary classification. However, there is a problem: How can I create compound (stoichiometry) specific features? For example, now the (70% A-30% B) compound becomes an \"independent\" training example and ideally should have at least some specific descriptors to distinguish it from the (60% A-40% B) compound in the same stability vector. Using my intuition I designed features such as differences |featureA-featureB| and weighted averages 0.1*featureA+0.9*featureB for the (10% A-90% B) compound. Then I discovered that there was a rich literature in this field. For details and references see the section Feature Engineering below.  \n",
    "\n",
    "__The reformulation of the problem described in the notebook has several advantages__: <br> \n",
    "- We have more training examples; to be established if they really contain more information\n",
    "- Differently from multilabel classification, there is plenty of algorithms that can be used as binary classifier\n",
    "- Introducing compound specific features might boost the \"accuracy\"\n",
    "- The problem of the swapping of the elements I previously analized is less crucial in this model\n",
    "- In a way this reformulation feels more natural\n",
    "\n",
    "__On the other side there are also drawbacks in the binary classifier formulation.__ I will discuss them in the conclusions and they are the reason why I dropped this approach in the final prediction. \n",
    "\n",
    "__Important:__ Following Ubaru et al. PRB 95, 214102 (2017) I introduce here some new elemental properties which were missing in the data provided: Cohesive energy, enthalpy of vaporization and electron affinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the training set with Pandas\n",
    "\n",
    "train=pd.read_csv(\"training_data.csv\") # creating the train dataframe\n",
    "\n",
    "#Following Ubaru et al. PRB 95, 214102 (2017)\n",
    "#I introduce here some new elemental properties which were missing in the data provided\n",
    "#Cohesive energy, Enthalpy of Vaporization and Electron Affinity.\n",
    "#Source of the data http://phases.imet-db.ru/elements/main.aspx (I checked some data and it seems reliable)\n",
    "\n",
    "cohesiveE=pd.read_csv(\"cohesive_energy.csv\")            # Loading cohesive energies for each element\n",
    "enthalpy_vap=pd.read_csv(\"enthalpy_vaporization.csv\")   # Loading enthalpy of vaporization for each element\n",
    "elec_affinity=pd.read_csv(\"electron_affinity.csv\")      # Loading electron affinity for each element\n",
    "\n",
    "# For convenience I'm setting the index of the dataframe to be the atom label\n",
    "cohesiveE=cohesiveE.set_index('atom')\n",
    "enthalpy_vap=enthalpy_vap.set_index('atom')\n",
    "elec_affinity=elec_affinity.set_index('atom')\n",
    "\n",
    "# For convenience the series is transformed into a dictionary\n",
    "cohesiveE_dic=cohesiveE['energy'].to_dict()\n",
    "enthalpy_vap_dic=enthalpy_vap['energy'].to_dict()\n",
    "elec_affinity_dic=elec_affinity['energy'].to_dict()\n",
    "\n",
    "# I create the new features columns; at this stage these columns\n",
    "# contain only the atoms name\n",
    "train[\"formulaA_cohesiveE\"]=train[\"formulaA\"]\n",
    "train[\"formulaB_cohesiveE\"]=train[\"formulaB\"]\n",
    "train[\"formulaA_enthalpy_vap\"]=train[\"formulaA\"]\n",
    "train[\"formulaB_enthalpy_vap\"]=train[\"formulaB\"]\n",
    "train[\"formulaA_elec_affinity\"]=train[\"formulaA\"]\n",
    "train[\"formulaB_elec_affinity\"]=train[\"formulaB\"]\n",
    "\n",
    "# Using the dictionaries I replace in the previously created columns the\n",
    "# name of the element with its corresponding property\n",
    "train=train.replace({\"formulaA_cohesiveE\": cohesiveE_dic})\n",
    "train=train.replace({\"formulaB_cohesiveE\": cohesiveE_dic})\n",
    "train=train.replace({\"formulaA_enthalpy_vap\": enthalpy_vap_dic})\n",
    "train=train.replace({\"formulaB_enthalpy_vap\": enthalpy_vap_dic})\n",
    "train=train.replace({\"formulaA_elec_affinity\": elec_affinity_dic})\n",
    "train=train.replace({\"formulaB_elec_affinity\": elec_affinity_dic})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Below some manipulations that will be useful later.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating 2 lists that contain the name of the features (columns) corresponding\n",
    "# to element A and B\n",
    "\n",
    "columns_A = [] # List that will contain the name of the columns containing the descriptors of element A\n",
    "columns_B = [] # Same thing for element B\n",
    "\n",
    "for col in list(train.columns):\n",
    "    if ('formulaA' in col):\n",
    "        columns_A.append(col)\n",
    "    if ('formulaB' in col):\n",
    "        columns_B.append(col)\n",
    "\n",
    "# Adding avg_coordination_A (or B) and avg_nearest_neighbor_distance_A (or B) that do not contain\n",
    "# the string formulaA or formulaB\n",
    "columns_A.append('avg_coordination_A')\n",
    "columns_A.append('avg_nearest_neighbor_distance_A')\n",
    "\n",
    "columns_B.append('avg_coordination_B')\n",
    "columns_B.append('avg_nearest_neighbor_distance_B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The labels to predict are strings of the form \"[1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0]\"\n",
    "# Below I will extract the corresponding list\n",
    "\n",
    "import ast\n",
    "\n",
    "list_label = train['stabilityVec'].tolist()    # A list that contains the column 'stabilityVec'\n",
    "list_label = map(ast.literal_eval,list_label)\n",
    "list_label_np = np.asarray(list_label)\n",
    "\n",
    "#Let's drop the stability index of the pure element, as we do not need to predict it\n",
    "list_label_compound = list_label_np[:,1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training and hold out test set\n",
    "\n",
    "I want to avoid to split the same element pair between taining and hold out test set. For this reason I do it here before feature engineering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the provided labeled data into training and hold out test set \n",
    "\n",
    "# Importing sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_tot = train  \n",
    "Y_tot = list_label_compound \n",
    "\n",
    "# Holding out 10% of the data to test the model \n",
    "x_train, x_test, y_train, y_test = train_test_split(X_tot, Y_tot, test_size=0.1, random_state=0)\n",
    "# x_test corresponds to the hold out set\n",
    "\n",
    "# Not shown here but I verified a posteriori that this splitting is well stratified both for \n",
    "# what concerns the stoichiometric classes and the stable/non-stable labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unrolling the labels\n",
    "\n",
    "The vector of stability diagrams [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.] [ 0.  0.  0.  1.  0.  0.  0.  0.  0.] [ 0.  0.  0.  0.  0.  0.  0.  0.  1.],.... becomes a single stability vector [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 0.  0.  0.  1.  0.  0.  0.  0.  0. 0.  0.  0.  0.  0.  0.  0.  0.  1. .............."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim_train = y_train.shape[0]*y_train.shape[1]\n",
    "Y_train_unroll = reshape(y_train,(dim_train))\n",
    "\n",
    "\n",
    "dim_test = y_test.shape[0]*y_test.shape[1]\n",
    "Y_test_unroll = reshape(y_test,(dim_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering \n",
    "\n",
    "Here I create compound (stoichiometry) specific features; fo the moment I will keep also the original features corresponding to element A and element B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These are the original features but each one of them is now\n",
    "# repeated 9 times (the number of labels)\n",
    "\n",
    "coltot = list(x_train.columns)\n",
    "\n",
    "train_dic = {}\n",
    "for i in coltot:\n",
    "    col_tmp = list(x_train[i])\n",
    "    lst = []\n",
    "    for j in col_tmp:\n",
    "        lst.extend([j]*9)\n",
    "    train_dic[i]=lst\n",
    "\n",
    "#################################################################### \n",
    "    \n",
    "# These are 8 columns containing dummy variables corresponding\n",
    "# to the different mixture classes (stoichiometries) 90%-10%, 80%-20%, etc. \n",
    "# As usually done with one-hot-encoded variables, 8 classes are sufficient to\n",
    "# describe 9 labels\n",
    "    \n",
    "for i in range(8):\n",
    "    col_name = \"mixing_class_\"+str(i)\n",
    "    col_tmp = list(x_train['formulaA'])\n",
    "    lst = [] \n",
    "    lst_tmp=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    lst_tmp[i]=1.0\n",
    "    for j in range(len(col_tmp)):\n",
    "        lst.extend(lst_tmp)\n",
    "    train_dic[col_name]=lst \n",
    "    \n",
    "#################################################################### \n",
    "\n",
    "# I'm introducing two columns that contain the actual\n",
    "# mixing percentage: For example for a 90%-10% mixture\n",
    "# we will have a column percentage_A with 0.9 and a column\n",
    "# prcentage_A with 0.1\n",
    "    \n",
    "for i in [\"A\",\"B\"]:\n",
    "    col_name = \"percentage_\"+str(i)\n",
    "    col_tmp = list(x_train['formulaA'])\n",
    "    lst = [] \n",
    "    if i==\"A\":\n",
    "        lst_tmp=[0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "    elif i==\"B\":\n",
    "        lst_tmp=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    for j in range(len(col_tmp)):\n",
    "        lst.extend(lst_tmp)\n",
    "    train_dic[col_name]=lst \n",
    "\n",
    "#################################################################### \n",
    "    \n",
    "# The previous data were created as a dictionary that I will trnasform in dataframe    \n",
    "X_train_unroll = pd.DataFrame(train_dic)    \n",
    "\n",
    "#################################################################### \n",
    "\n",
    "# Features correponding to the L^p norms of the stoichiometry\n",
    "# These are the stoichiometric attributes in supplementary material of \n",
    "# Ward et al. npj Computational Materials 2, 16028 (2016).\n",
    "# Actually, for this problem these features are not very helpful\n",
    "for i in range(2,11):\n",
    "    col_name=\"norm_stoichiometric_\"+str(i)\n",
    "    X_train_unroll[col_name]=(abs(X_train_unroll[\"percentage_A\"])**i+abs(X_train_unroll[\"percentage_B\"])**i)**(1./i)\n",
    "\n",
    "#################################################################### \n",
    "\n",
    "# I (re)discovered the differences and weighted average features on my own\n",
    "# However, they were already used together with average deviation\n",
    "# in Ward et al. npj Computational Materials 2, 16028 (2016).\n",
    "\n",
    "for i in range(1,len(columns_A)): \n",
    "    col_name_diff = \"diff_\"+columns_A[i]\n",
    "    col_name_mean = \"mean_\"+columns_A[i]\n",
    "    col_name_dev = \"dev_\"+columns_A[i]\n",
    "    X_train_unroll[col_name_diff]=abs(X_train_unroll[columns_A[i]]-X_train_unroll[columns_B[i]])\n",
    "    X_train_unroll[col_name_mean]=(X_train_unroll[\"percentage_A\"]*X_train_unroll[columns_A[i]]\n",
    "                                   +X_train_unroll[\"percentage_B\"]*X_train_unroll[columns_B[i]])\n",
    "    X_train_unroll[col_name_dev]=(X_train_unroll[\"percentage_A\"]*\n",
    "                                  abs(X_train_unroll[columns_A[i]]-X_train_unroll[col_name_mean])\n",
    "                                   +X_train_unroll[\"percentage_B\"]*\n",
    "                                  abs(X_train_unroll[columns_B[i]]-X_train_unroll[col_name_mean]))\n",
    " \n",
    "#################################################################### \n",
    " \n",
    "#The following features correspond to the valence orbital occupation attributes \n",
    "#These features have been proposed in Meredig et al. PRB 89 094104 (2014)\n",
    "\n",
    "X_train_unroll[\"Fs\"]=((X_train_unroll[\"percentage_A\"]*X_train_unroll[\"formulaA_elements_NsValence\"]\n",
    "                       +X_train_unroll[\"percentage_B\"]*X_train_unroll[\"formulaB_elements_NsValence\"])/\n",
    "                      (X_train_unroll[\"percentage_A\"]*X_train_unroll[\"formulaA_elements_NValance\"]\n",
    "                       +X_train_unroll[\"percentage_B\"]*X_train_unroll[\"formulaB_elements_NValance\"]))\n",
    "\n",
    "X_train_unroll[\"Fp\"]=((X_train_unroll[\"percentage_A\"]*X_train_unroll[\"formulaA_elements_NpValence\"]\n",
    "                       +X_train_unroll[\"percentage_B\"]*X_train_unroll[\"formulaB_elements_NpValence\"])/\n",
    "                      (X_train_unroll[\"percentage_A\"]*X_train_unroll[\"formulaA_elements_NValance\"]\n",
    "                       +X_train_unroll[\"percentage_B\"]*X_train_unroll[\"formulaB_elements_NValance\"]))\n",
    "                      \n",
    "X_train_unroll[\"Fd\"]=((X_train_unroll[\"percentage_A\"]*X_train_unroll[\"formulaA_elements_NdValence\"]\n",
    "                       +X_train_unroll[\"percentage_B\"]*X_train_unroll[\"formulaB_elements_NdValence\"])/\n",
    "                      (X_train_unroll[\"percentage_A\"]*X_train_unroll[\"formulaA_elements_NValance\"]\n",
    "                       +X_train_unroll[\"percentage_B\"]*X_train_unroll[\"formulaB_elements_NValance\"]))            \n",
    "                      \n",
    "X_train_unroll[\"Ff\"]=((X_train_unroll[\"percentage_A\"]*X_train_unroll[\"formulaA_elements_NfValence\"]\n",
    "                       +X_train_unroll[\"percentage_B\"]*X_train_unroll[\"formulaB_elements_NfValence\"])/\n",
    "                      (X_train_unroll[\"percentage_A\"]*X_train_unroll[\"formulaA_elements_NValance\"]\n",
    "                       +X_train_unroll[\"percentage_B\"]*X_train_unroll[\"formulaB_elements_NValance\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hold out test set__ Same as before for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coltot = list(x_test.columns)\n",
    "\n",
    "test_dic = {}\n",
    "for i in coltot:\n",
    "    col_tmp = list(x_test[i])\n",
    "    lst = []\n",
    "    for j in col_tmp:\n",
    "        lst.extend([j]*9)\n",
    "    test_dic[i]=lst  \n",
    "\n",
    "#################################################################### \n",
    "\n",
    "for i in range(8):\n",
    "    col_name = \"mixing_class_\"+str(i)\n",
    "    col_tmp = list(x_test['formulaA'])\n",
    "    lst = [] \n",
    "    lst_tmp=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    lst_tmp[i]=1.0\n",
    "    for j in range(len(col_tmp)):\n",
    "        lst.extend(lst_tmp)\n",
    "    test_dic[col_name]=lst   \n",
    "    \n",
    "#################################################################### \n",
    "    \n",
    "for i in [\"A\",\"B\"]:\n",
    "    col_name = \"percentage_\"+str(i)\n",
    "    col_tmp = list(x_test['formulaA'])\n",
    "    lst = [] \n",
    "    if i==\"A\":\n",
    "        lst_tmp=[0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "    elif i==\"B\":\n",
    "        lst_tmp=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    for j in range(len(col_tmp)):\n",
    "        lst.extend(lst_tmp)\n",
    "    test_dic[col_name]=lst \n",
    "    \n",
    "#################################################################### \n",
    "\n",
    "X_test_unroll = pd.DataFrame(test_dic)    \n",
    "\n",
    "#################################################################### \n",
    "\n",
    "for i in range(2,11):\n",
    "    col_name=\"norm_stoichiometric_\"+str(i)\n",
    "    X_test_unroll[col_name]=(abs(X_test_unroll[\"percentage_A\"])**i+abs(X_test_unroll[\"percentage_B\"])**i)**(1./i)\n",
    "\n",
    "#################################################################### \n",
    "\n",
    "for i in range(1,len(columns_A)): \n",
    "    col_name_diff = \"diff_\"+columns_A[i]\n",
    "    col_name_mean = \"mean_\"+columns_A[i]\n",
    "    col_name_dev = \"dev_\"+columns_A[i]\n",
    "    X_test_unroll[col_name_diff]=abs(X_test_unroll[columns_A[i]]-X_test_unroll[columns_B[i]])\n",
    "    X_test_unroll[col_name_mean]=(X_test_unroll[\"percentage_A\"]*X_test_unroll[columns_A[i]]\n",
    "                                   +X_test_unroll[\"percentage_B\"]*X_test_unroll[columns_B[i]])\n",
    "    X_test_unroll[col_name_dev]=(X_test_unroll[\"percentage_A\"]*\n",
    "                                  abs(X_test_unroll[columns_A[i]]-X_test_unroll[col_name_mean])\n",
    "                                   +X_test_unroll[\"percentage_B\"]*\n",
    "                                  abs(X_test_unroll[columns_B[i]]-X_test_unroll[col_name_mean]))\n",
    "    \n",
    "#################################################################### \n",
    "    \n",
    "X_test_unroll[\"Fs\"]=((X_test_unroll[\"percentage_A\"]*X_test_unroll[\"formulaA_elements_NsValence\"]\n",
    "                       +X_test_unroll[\"percentage_B\"]*X_test_unroll[\"formulaB_elements_NsValence\"])/\n",
    "                      (X_test_unroll[\"percentage_A\"]*X_test_unroll[\"formulaA_elements_NValance\"]\n",
    "                       +X_test_unroll[\"percentage_B\"]*X_test_unroll[\"formulaB_elements_NValance\"]))\n",
    "\n",
    "X_test_unroll[\"Fp\"]=((X_test_unroll[\"percentage_A\"]*X_test_unroll[\"formulaA_elements_NpValence\"]\n",
    "                       +X_test_unroll[\"percentage_B\"]*X_test_unroll[\"formulaB_elements_NpValence\"])/\n",
    "                      (X_test_unroll[\"percentage_A\"]*X_test_unroll[\"formulaA_elements_NValance\"]\n",
    "                       +X_test_unroll[\"percentage_B\"]*X_test_unroll[\"formulaB_elements_NValance\"]))\n",
    "                      \n",
    "X_test_unroll[\"Fd\"]=((X_test_unroll[\"percentage_A\"]*X_test_unroll[\"formulaA_elements_NdValence\"]\n",
    "                       +X_test_unroll[\"percentage_B\"]*X_test_unroll[\"formulaB_elements_NdValence\"])/\n",
    "                      (X_test_unroll[\"percentage_A\"]*X_test_unroll[\"formulaA_elements_NValance\"]\n",
    "                       +X_test_unroll[\"percentage_B\"]*X_test_unroll[\"formulaB_elements_NValance\"]))            \n",
    "                      \n",
    "X_test_unroll[\"Ff\"]=((X_test_unroll[\"percentage_A\"]*X_test_unroll[\"formulaA_elements_NfValence\"]\n",
    "                       +X_test_unroll[\"percentage_B\"]*X_test_unroll[\"formulaB_elements_NfValence\"])/\n",
    "                      (X_test_unroll[\"percentage_A\"]*X_test_unroll[\"formulaA_elements_NValance\"]\n",
    "                       +X_test_unroll[\"percentage_B\"]*X_test_unroll[\"formulaB_elements_NValance\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "At this point I have 281 features. To avoid overfitting and spare computer time I included only 100 features selected by the random forest algorithm (not shown). With the exception of the Mendeleev Number all the other features are \n",
    "engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# A list of the features that I will keep\n",
    "# mixing_class_0, mixing_class_1, mixing_class_2 etc. are not among the most important for the random forest\n",
    "# classifier but they are necessary to obtain reasonable results with the SVC  \n",
    "col_to_keep = [\"dev_formulaA_elements_NpUnfilled\", \"dev_formulaA_elements_Column\", \n",
    "               \"dev_formulaA_elements_NdValence\", \"mixing_class_0\", \n",
    "               \"mixing_class_1\", \"mixing_class_2\", \"mixing_class_3\", \n",
    "               \"mixing_class_4\", \"mixing_class_5\", \"mixing_class_6\", \n",
    "               \"mixing_class_7\", \"mean_formulaA_elements_Column\", \n",
    "               \"diff_formulaA_elements_GSenergy_pa\", \"mean_formulaA_elements_MendeleevNumber\", \n",
    "               \"mean_formulaA_elements_FirstIonizationEnergy\", \"dev_formulaA_elements_GSenergy_pa\", \n",
    "               \"dev_formulaA_elements_AtomicVolume\", \"dev_formulaA_elements_HeatCapacityMolar\", \n",
    "               \"mean_formulaA_elements_HHIp\", \"dev_formulaA_elec_affinity\", \n",
    "               \"dev_formulaA_elements_ElectronSurfaceDensityWS\", \"mean_formulaA_elements_HeatCapacityMolar\", \n",
    "               \"mean_formulaA_elec_affinity\", \"mean_formulaA_elements_GSbandgap\", \n",
    "               \"diff_formulaA_elements_HeatCapacityMolar\", \"mean_formulaA_elements_IsFBlock\", \n",
    "               \"percentage_B\", \"mean_formulaA_elements_MiracleRadius\", \n",
    "               \"mean_formulaA_elements_Polarizability\", \"percentage_A\", \n",
    "               \"mean_formulaA_elements_ElectronSurfaceDensityWS\", \n",
    "               \"diff_formulaA_elements_ElectronSurfaceDensityWS\", \n",
    "               \"mean_formulaA_elements_CovalentRadius\", \"mean_formulaA_elements_Electronegativity\", \n",
    "               \"mean_formulaA_elements_BulkModulus\", \"dev_formulaA_elements_MendeleevNumber\", \n",
    "               \"mean_formulaA_elements_HeatCapacityMass\", \"diff_formulaA_elements_AtomicVolume\", \n",
    "               \"dev_avg_nearest_neighbor_distance_A\", \"dev_formulaA_elements_MeltingT\", \n",
    "               \"mean_formulaA_elements_GSenergy_pa\", \"mean_formulaA_elements_HHIr\", \n",
    "               \"dev_formulaA_elements_HeatFusion\", \"mean_formulaA_elements_NUnfilled\", \n",
    "               \"mean_formulaA_elements_Density\", \"dev_formulaA_elements_Electronegativity\", \n",
    "               \"dev_formulaA_elements_FirstIonizationEnergy\", \"mean_formulaA_elements_NpUnfilled\", \n",
    "               \"dev_formulaA_elements_HHIr\", \"Fd\", \"dev_formulaA_elements_BulkModulus\", \n",
    "               \"dev_formulaA_elements_MiracleRadius\", \"dev_formulaA_cohesiveE\", \n",
    "               \"dev_formulaA_elements_BoilingT\", \"mean_formulaA_enthalpy_vap\", \n",
    "               \"mean_formulaA_elements_BoilingT\", \"dev_formulaA_elements_HHIp\", \n",
    "               \"mean_formulaA_elements_SpaceGroupNumber\", \"dev_formulaA_enthalpy_vap\", \n",
    "               \"mean_formulaA_elements_HeatFusion\", \"mean_formulaA_elements_AtomicVolume\", \"Fs\", \n",
    "               \"mean_formulaA_elements_ShearModulus\", \"dev_formulaA_elements_Density\", \n",
    "               \"mean_formulaA_cohesiveE\", \"dev_formulaA_elements_CovalentRadius\", \n",
    "               \"dev_formulaA_elements_HeatCapacityMass\", \"diff_formulaA_elements_MiracleRadius\", \n",
    "               \"dev_formulaA_elements_SpaceGroupNumber\", \"diff_formulaA_elements_HeatFusion\", \n",
    "               \"mean_formulaA_elements_MeltingT\", \"mean_formulaA_elements_NpValence\", \n",
    "               \"mean_avg_nearest_neighbor_distance_A\", \"diff_formulaA_cohesiveE\", \n",
    "               \"dev_formulaA_elements_GSvolume_pa\", \"dev_formulaA_elements_Polarizability\", \n",
    "               \"dev_formulaA_elements_GSestBCClatcnt\", \"dev_formulaA_elements_ShearModulus\", \n",
    "               \"dev_avg_coordination_A\", \"diff_avg_nearest_neighbor_distance_A\", \n",
    "               \"mean_formulaA_elements_NValance\", \"mean_formulaA_elements_AtomicWeight\", \n",
    "               \"mean_formulaA_elements_ICSDVolume\", \"Fp\", \"diff_formulaA_elec_affinity\", \n",
    "               \"dev_formulaA_elements_AtomicWeight\", \"mean_formulaA_elements_NdValence\", \n",
    "               \"dev_formulaA_elements_ICSDVolume\", \"mean_avg_coordination_A\", \n",
    "               \"mean_formulaA_elements_GSestBCClatcnt\", \"mean_formulaA_elements_Number\", \n",
    "               \"diff_formulaA_elements_MeltingT\", \"formulaA_elements_MendeleevNumber\", \n",
    "               \"formulaB_elements_MendeleevNumber\", \"mean_formulaA_elements_GSestFCClatcnt\", \n",
    "               \"diff_formulaA_elements_Column\", \"dev_formulaA_elements_GSestFCClatcnt\", \"Ff\", \n",
    "               \"diff_formulaA_elements_NpUnfilled\", \"mean_formulaA_elements_GSvolume_pa\"]\n",
    "\n",
    "X_train_unroll=X_train_unroll[col_to_keep]\n",
    "X_test_unroll=X_test_unroll[col_to_keep]\n",
    "\n",
    "X_train = X_train_unroll \n",
    "Y_train = Y_train_unroll \n",
    "\n",
    "X_test = X_test_unroll \n",
    "Y_test = Y_test_unroll "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing a random forest classifier\n",
    "\n",
    "By using 10-fold cross validation on the training set I performed a grid search for the best parameters for the random forest classifier. I used as scoring function the F1 function. Optimizing this metric leads to a good compromise between precision and recall. This is not exactly like training on subset accuracy (accuracy on the full stability vector) but it's a reasonable choice. The code lines corresponding to the grid search take a lot of time to execute and have been commented. With the optimized parameters I then evaluate the performance on the hold out (test) set.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the training set.\n",
      "The scores are computed on the training set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00     18590\n",
      "        1.0       1.00      1.00      1.00      2236\n",
      "\n",
      "avg / total       1.00      1.00      1.00     20826\n",
      "\n",
      "\n",
      "Subset accuracy (accuracy on full stability vector) 1.0\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the training set.\n",
      "The scores are computed on the hold out test set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.94      0.99      0.96      2065\n",
      "        1.0       0.85      0.50      0.63       257\n",
      "\n",
      "avg / total       0.93      0.93      0.93      2322\n",
      "\n",
      "\n",
      "Subset accuracy (accuracy on full stability vector) 0.639534883721\n"
     ]
    }
   ],
   "source": [
    "# Importing sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import coverage_error \n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "################################################################################\n",
    "\n",
    "####### The following commented lines have been used to optimize the RandomForestClassifier parameters\n",
    "####### This takes some time \n",
    "\n",
    "## Important parameters to optimize: n_estimators, max_features, and max_depth\n",
    "\n",
    "# search_grid_rf = [{'n_estimators': [50, 100, 150, 200, 250, 300],\n",
    "#                  'max_features': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "#                  'max_depth': [14, 18, 22, 26, 30]}] \n",
    "\n",
    "# clf_rf = GridSearchCV(RandomForestClassifier(random_state=0), search_grid_rf, cv=10,\n",
    "#                         scoring='f1', verbose=10, n_jobs=4)\n",
    "\n",
    "# clf_rf.fit(X_train, Y_train)\n",
    "\n",
    "# print \"Best parameters set found on traning set:\"\n",
    "# print \"\"\n",
    "# print clf_rf.best_params_\n",
    "# print \"\"\n",
    "# print \"Grid scores on training set:\"\n",
    "# print \"\"\n",
    "\n",
    "# means = clf_rf.cv_results_['mean_test_score']\n",
    "# stds = clf_rf.cv_results_['std_test_score']\n",
    "# for mean, std, params in zip(means, stds, clf_rf.cv_results_['params']):\n",
    "#     print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "#             % (mean, std * 2, params))\n",
    "# print \"\"\n",
    "\n",
    "# Best parameters set found on traning set:\n",
    "\n",
    "# {'max_features': 0.5, 'n_estimators': 100, 'max_depth': 26}\n",
    "\n",
    "# 0.553 (+/-0.073) for {'max_features': 0.5, 'n_estimators': 100, 'max_depth': 26}\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# The random forest classifier\n",
    "clf_rf = RandomForestClassifier(max_features = 0.5, n_estimators = 100, max_depth = 26, random_state=0)\n",
    "# With the parameters above I got 0.553 (+/-0.073) as best score (F1) in the grid search\n",
    "# random_state=0 allows for reproducibility; alternatively we could release this constraint \n",
    "# and average over different final results\n",
    "\n",
    "###########################\n",
    "\n",
    "# Evaluating predictions on training set\n",
    "clf_rf.fit(X_train, Y_train)\n",
    "predictions_train = clf_rf.predict(X_train)\n",
    "\n",
    "print \"Detailed classification report:\"\n",
    "print \"\"\n",
    "print \"The model is trained on the training set.\"\n",
    "print(\"The scores are computed on the training set.\")\n",
    "print \"\"\n",
    "print(classification_report(Y_train, predictions_train))\n",
    "print \"\"\n",
    "\n",
    "predictions_train = clf_rf.predict(X_train)\n",
    "\n",
    "#In order to evaluate the subset accuracy I need to reshape the arrays\n",
    "lenr = len(Y_train)/9\n",
    "Y_train_reshape = reshape(Y_train,(lenr,9))\n",
    "predictions_train_reshape = reshape(predictions_train,(lenr,9))\n",
    "\n",
    "print \"Subset accuracy (accuracy on full stability vector)\", accuracy_score(Y_train, predictions_train)\n",
    "\n",
    "###########################\n",
    "\n",
    "# Evaluating predictions on hold out test set\n",
    "predictions_test = clf_rf.predict(X_test)\n",
    "\n",
    "print \"Detailed classification report:\"\n",
    "print \"\"\n",
    "print \"The model is trained on the training set.\"\n",
    "print(\"The scores are computed on the hold out test set.\")\n",
    "print \"\"\n",
    "print(classification_report(Y_test, predictions_test))\n",
    "print \"\"\n",
    "\n",
    "lenr = len(Y_test)/9\n",
    "Y_test_reshape = reshape(Y_test,(lenr,9))\n",
    "predictions_test_reshape = reshape(predictions_test,(lenr,9))\n",
    "\n",
    "print \"Subset accuracy (accuracy on full stability vector)\", accuracy_score(Y_test_reshape, predictions_test_reshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remarks on random forest classifier__ <br>\n",
    "- In 10-fold cross validation I obtained 0.553 (+/-0.073) as best F1 score. It is important to notice that the 10 folds can divide the element pairs; for example, the compound (90%A-10%B) can be in one fold and (80%A-20%B) can be in a different one. \n",
    "- On the hold out test set I obtain a 0.63 F1 score with a precision that is sizeably higher than the recall. The subset accuracy on the hold out test set is 0.64.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector classifier\n",
    "\n",
    "By using 10-fold cross validation on the training set I performed a grid search for the best parameters for the support vector classifier with a Gaussian kernel. I used as scoring function the F1 function. Optimizing this metric leads to a good compromise between precision and recall. This is not exactly like training on subset accuracy (accuracy on the full stability vector) but it's a reasonable choice. The code lines corresponding to the grid search take a lot of time to execute and have been commented. With the optimized parameters I then evaluate the performance on the hold out (test) set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the training set.\n",
      "The scores are computed on the hold out test set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.96      0.97      0.96      2065\n",
      "        1.0       0.72      0.66      0.69       257\n",
      "\n",
      "avg / total       0.93      0.93      0.93      2322\n",
      "\n",
      "\n",
      "Subset accuracy (accuracy on full stability vector) 0.635658914729\n"
     ]
    }
   ],
   "source": [
    "# Importing sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Differently from random forests SVC requires feature scaling\n",
    "# To prevent data leakage I fit the scaler only to X_train\n",
    "\n",
    "scaler=MaxAbsScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "####### The following commented lines have been used to optimize the SVC parameters\n",
    "####### This is faster than random forest (only 2 parameters)\n",
    "\n",
    "## Parameters to optimize: C and gamma\n",
    "# search_grid_svc = [{'C': [0.1, 1.0, 5.0, 10.0, 20.0, 60.0, 80.0, 100.0, 120.0, 150.0, 200.0, 300.0], \n",
    "#                  'gamma': [0.0005, 0.001, 0.002, 0.005, 0.008, 0.01, 0.05, 0.1, 1.0]}] \n",
    "\n",
    "# clf_svc = GridSearchCV(SVC(random_state=0), search_grid_svc, cv=10,\n",
    "#                         scoring='f1', verbose=10, n_jobs=4)\n",
    "\n",
    "# clf_svc.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# print \"Best parameters set found on traning set:\"\n",
    "# print \"\"\n",
    "# print clf_svc.best_params_\n",
    "# print \"\"\n",
    "# print \"Grid scores on training set:\"\n",
    "# print \"\"\n",
    "\n",
    "# means = clf_svc.cv_results_['mean_test_score']\n",
    "# stds = clf_svc.cv_results_['std_test_score']\n",
    "# for mean, std, params in zip(means, stds, clf_svc.cv_results_['params']):\n",
    "#     print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "#             % (mean, std * 2, params))\n",
    "# print \"\"\n",
    "\n",
    "# Best parameters set found on traning set:\n",
    "\n",
    "# {'C': 100.0, 'gamma': 0.1}\n",
    "\n",
    "# 0.622 (+/-0.045) for {'C': 100.0, 'gamma': 0.1}\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# The classifier\n",
    "clf_svc = SVC(C=100.0, gamma=0.1, random_state=0)\n",
    "# With the parameters above a got 0.622 (+/-0.045) best score in the grid search\n",
    "\n",
    "# Fitting the classifier\n",
    "clf_svc.fit(X_train_scaled, Y_train)\n",
    "\n",
    "predictions_svc = clf_svc.predict(X_test_scaled)\n",
    "\n",
    "# Printing precision, recall, etc. with respect to the test set \n",
    "print \"Detailed classification report:\"\n",
    "print \"\"\n",
    "print \"The model is trained on the training set.\"\n",
    "print(\"The scores are computed on the hold out test set.\")\n",
    "print \"\"\n",
    "print(classification_report(Y_test, predictions_svc))\n",
    "print \"\"\n",
    "\n",
    "lenr = len(Y_test)/9\n",
    "Y_test_reshape = reshape(Y_test,(lenr,9))\n",
    "predictions_svc_reshape = reshape(predictions_svc,(lenr,9))\n",
    "\n",
    "print \"Subset accuracy (accuracy on full stability vector)\", accuracy_score(Y_test_reshape, predictions_svc_reshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remarks on SVC__ <br>\n",
    "- In 10-fold cross validation I obtained 0.622 (+/-0.045) as best F1 score. It is important to notice that the 10 folds can divide the element pairs; for example, the compound (90%A-10%B) can be in one fold and (80%A-20%B) can be in a different one. \n",
    "- On the hold out test set I obtain a 0.69 F1 score with a precision that is comparable to recall. The subset accuracy on the hold out test set is 0.635, slightly worst than RF. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network classifier\n",
    "\n",
    "I used Keras to create a fully connected neural network model. By using 10-fold cross validation on the training set I performed a grid search for the best parameters for the random forest classifier. As the grid search with 10-fold cross validation is computationally very demanding I limited the number of layers.\n",
    "I used as scoring function the F1 function. Optimizing this metric leads to a good compromise between precision and recall. This is not exactly like training on subset accuracy (accuracy on the full stability vector) but it's a reasonable choice. The code lines corresponding to the grid search take a lot of time to execute and have been commented. With the optimized parameters I then evaluate the performance on the hold out (test) set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# The model; n_labels and dim have the correct default labels (a bit hard coded)\n",
    "def nn_model(n_layers=1, n_neuron1=1, n_neuron2=1, drpout=0.0, n_labels=1, dim=100):\n",
    "    # create nn model\n",
    "    \n",
    "    # two layers maximum\n",
    "    if (n_layers>2):\n",
    "        return -1\n",
    "    \n",
    "    # sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # layer 1\n",
    "    model.add(Dense(n_neuron1, input_dim=dim, activation='relu'))\n",
    "    model.add(Dropout(drpout))\n",
    "    \n",
    "    # layer 2\n",
    "    if n_layers == 2:\n",
    "        model.add(Dense(n_neuron2, activation='relu'))\n",
    "    model.add(Dropout(drpout))\n",
    "    \n",
    "    # output layer \n",
    "    model.add(Dense(n_labels, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# To interface Keras and Scikit-learn I need to wrap the Keras model with KerasClassifier\n",
    "model = KerasClassifier(build_fn=nn_model, epochs=50, batch_size=16, verbose=0)\n",
    "\n",
    "# Differently from random forests NN require feature scaling\n",
    "# To prevent data leakage I fit the scaler only to X_train\n",
    "scaler=MaxAbsScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "####### The following commented lines have been used to optimize the NN parameters\n",
    "####### Extremely slow\n",
    "####### I fixed the number of epochs to 50 \n",
    "\n",
    "#search_grid = [{'n_layers': [1], \n",
    "#                'n_neuron1': [20, 60, 100, 150, 200, 250, 300], \n",
    "#                'n_neuron2': [20, 60, 100, 150, 200, 250, 300], \n",
    "#                'drpout': [0.0, 0.1, 0.2, 0.3, 0.4]}]\n",
    "#clf_nn = GridSearchCV(estimator=model, param_grid=search_grid, cv=10, verbose=10, n_jobs=4,\n",
    "#                        scoring='f1')\n",
    "#clf_nn.fit(X_train_scaled, Y_train)\n",
    "\n",
    "#print \"Best parameters set found on traning set:\"\n",
    "#print \"\"\n",
    "#print clf_nn.best_params_\n",
    "#print \"\"\n",
    "#print \"Grid scores on training set:\"\n",
    "#print \"\"\n",
    "\n",
    "#means = clf_nn.cv_results_['mean_test_score']\n",
    "#stds = clf_nn.cv_results_['std_test_score']\n",
    "#for mean, std, params in zip(means, stds, clf_nn.cv_results_['params']):\n",
    "#    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "#            % (mean, std * 2, params))\n",
    "#print \"\"\n",
    "\n",
    "\n",
    "#Best parameters set found on traning set:\n",
    "\n",
    "#{'drpout': 0.0, 'n_neuron1': 150, 'n_layers': 1}\n",
    "\n",
    "#0.606 (+/-0.032) for {'drpout': 0.0, 'n_neuron1': 150, 'n_layers': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20826/20826 [==============================] - 2s 99us/step - loss: 0.2600\n",
      "Epoch 2/50\n",
      "20826/20826 [==============================] - 2s 81us/step - loss: 0.2330\n",
      "Epoch 3/50\n",
      "20826/20826 [==============================] - 2s 81us/step - loss: 0.2215\n",
      "Epoch 4/50\n",
      "20826/20826 [==============================] - 2s 82us/step - loss: 0.2118\n",
      "Epoch 5/50\n",
      "20826/20826 [==============================] - 2s 84us/step - loss: 0.2061\n",
      "Epoch 6/50\n",
      "20826/20826 [==============================] - 2s 87us/step - loss: 0.1997\n",
      "Epoch 7/50\n",
      "20826/20826 [==============================] - 2s 85us/step - loss: 0.1953\n",
      "Epoch 8/50\n",
      "20826/20826 [==============================] - 2s 82us/step - loss: 0.1921\n",
      "Epoch 9/50\n",
      "20826/20826 [==============================] - 2s 81us/step - loss: 0.1883\n",
      "Epoch 10/50\n",
      "20826/20826 [==============================] - 2s 89us/step - loss: 0.1862\n",
      "Epoch 11/50\n",
      "20826/20826 [==============================] - 2s 89us/step - loss: 0.1831\n",
      "Epoch 12/50\n",
      "20826/20826 [==============================] - 2s 85us/step - loss: 0.1808\n",
      "Epoch 13/50\n",
      "20826/20826 [==============================] - 2s 91us/step - loss: 0.1776\n",
      "Epoch 14/50\n",
      "20826/20826 [==============================] - 2s 86us/step - loss: 0.1766\n",
      "Epoch 15/50\n",
      "20826/20826 [==============================] - 2s 87us/step - loss: 0.1725\n",
      "Epoch 16/50\n",
      "20826/20826 [==============================] - 2s 86us/step - loss: 0.1720\n",
      "Epoch 17/50\n",
      "20826/20826 [==============================] - 2s 84us/step - loss: 0.1690\n",
      "Epoch 18/50\n",
      "20826/20826 [==============================] - 2s 91us/step - loss: 0.1684\n",
      "Epoch 19/50\n",
      "20826/20826 [==============================] - 2s 92us/step - loss: 0.1652\n",
      "Epoch 20/50\n",
      "20826/20826 [==============================] - 2s 92us/step - loss: 0.1639\n",
      "Epoch 21/50\n",
      "20826/20826 [==============================] - 2s 90us/step - loss: 0.1620\n",
      "Epoch 22/50\n",
      "20826/20826 [==============================] - 2s 81us/step - loss: 0.1605\n",
      "Epoch 23/50\n",
      "20826/20826 [==============================] - 2s 80us/step - loss: 0.1592\n",
      "Epoch 24/50\n",
      "20826/20826 [==============================] - 2s 80us/step - loss: 0.1570\n",
      "Epoch 25/50\n",
      "20826/20826 [==============================] - 2s 82us/step - loss: 0.1566\n",
      "Epoch 26/50\n",
      "20826/20826 [==============================] - 2s 82us/step - loss: 0.1534\n",
      "Epoch 27/50\n",
      "20826/20826 [==============================] - 2s 90us/step - loss: 0.1524\n",
      "Epoch 28/50\n",
      "20826/20826 [==============================] - 2s 84us/step - loss: 0.1516\n",
      "Epoch 29/50\n",
      "20826/20826 [==============================] - 2s 81us/step - loss: 0.1512\n",
      "Epoch 30/50\n",
      "20826/20826 [==============================] - 2s 82us/step - loss: 0.1482\n",
      "Epoch 31/50\n",
      "20826/20826 [==============================] - 2s 80us/step - loss: 0.1477\n",
      "Epoch 32/50\n",
      "20826/20826 [==============================] - 2s 87us/step - loss: 0.1464\n",
      "Epoch 33/50\n",
      "20826/20826 [==============================] - 2s 82us/step - loss: 0.1451\n",
      "Epoch 34/50\n",
      "20826/20826 [==============================] - 2s 80us/step - loss: 0.1442\n",
      "Epoch 35/50\n",
      "20826/20826 [==============================] - 2s 81us/step - loss: 0.1425\n",
      "Epoch 36/50\n",
      "20826/20826 [==============================] - 2s 80us/step - loss: 0.1422\n",
      "Epoch 37/50\n",
      "20826/20826 [==============================] - ETA: 0s - loss: 0.140 - 2s 80us/step - loss: 0.1411\n",
      "Epoch 38/50\n",
      "20826/20826 [==============================] - 2s 79us/step - loss: 0.1376\n",
      "Epoch 39/50\n",
      "20826/20826 [==============================] - 2s 80us/step - loss: 0.1377\n",
      "Epoch 40/50\n",
      "20826/20826 [==============================] - 2s 79us/step - loss: 0.1368\n",
      "Epoch 41/50\n",
      "20826/20826 [==============================] - 2s 78us/step - loss: 0.1354\n",
      "Epoch 42/50\n",
      "20826/20826 [==============================] - 2s 79us/step - loss: 0.1335\n",
      "Epoch 43/50\n",
      "20826/20826 [==============================] - 2s 82us/step - loss: 0.1345\n",
      "Epoch 44/50\n",
      "20826/20826 [==============================] - 2s 80us/step - loss: 0.1332\n",
      "Epoch 45/50\n",
      "20826/20826 [==============================] - 2s 79us/step - loss: 0.1319\n",
      "Epoch 46/50\n",
      "20826/20826 [==============================] - 2s 79us/step - loss: 0.1297\n",
      "Epoch 47/50\n",
      "20826/20826 [==============================] - 2s 80us/step - loss: 0.1301\n",
      "Epoch 48/50\n",
      "20826/20826 [==============================] - 2s 79us/step - loss: 0.1293\n",
      "Epoch 49/50\n",
      "20826/20826 [==============================] - 2s 78us/step - loss: 0.1262\n",
      "Epoch 50/50\n",
      "20826/20826 [==============================] - 2s 79us/step - loss: 0.1255\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the training set.\n",
      "The scores are computed on the hold out test set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.95      0.97      0.96      2065\n",
      "        1.0       0.69      0.61      0.65       257\n",
      "\n",
      "avg / total       0.92      0.93      0.92      2322\n",
      "\n",
      "\n",
      "Subset accuracy (accuracy on full stability vector) 0.596899224806\n"
     ]
    }
   ],
   "source": [
    "# In this frame I evaluate the performance of the nn on the hold out test set\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Scaling features\n",
    "scaler=MaxAbsScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fitting the model\n",
    "clf_nn = nn_model(n_layers=1, n_neuron1=150, drpout=0.0)\n",
    "clf_nn.fit(X_train_scaled, Y_train, batch_size=16, epochs=50)\n",
    "\n",
    "# The nn predicts probabilities\n",
    "# I fix the threshold to 0.5 to make stability/instability predictions\n",
    "predictions_nn = clf_nn.predict(X_test_scaled)\n",
    "predictions_nn[predictions_nn>= 0.5] = 1\n",
    "predictions_nn[predictions_nn<0.5] = 0\n",
    "\n",
    "\n",
    "# Printing precision, recall, etc. with respect to the test set \n",
    "print \"Detailed classification report:\"\n",
    "print \"\"\n",
    "print \"The model is trained on the training set.\"\n",
    "print(\"The scores are computed on the hold out test set.\")\n",
    "print \"\"\n",
    "print(classification_report(Y_test, predictions_nn))\n",
    "print \"\"\n",
    "\n",
    "lenr = len(Y_test)/9\n",
    "Y_test_reshape = reshape(Y_test,(lenr,9))\n",
    "predictions_nn_reshape = reshape(predictions_nn,(lenr,9))\n",
    "\n",
    "print \"Subset accuracy (accuracy on full stability vector)\", accuracy_score(Y_test_reshape, predictions_nn_reshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remarks on SVC__ <br>\n",
    "- In 10-fold cross validation I obtained 0.606 (+/-0.032) as best F1 score. It is important to notice that the 10 folds can divide the element pairs; for example, the compound (90%A-10%B) can be in one fold and (80%A-20%B) can be in a different one. \n",
    "- On the hold out test set I obtain a 0.65 F1 score with a precision that is comparable to recall. The subset accuracy on the hold out test set is about 0.6, worst than RF and SVC.\n",
    "- No random seed fixed: The results fluctuate a bit rerunning the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Conclusions \n",
    "\n",
    "I trained RF, SVC, and NN on the binary classification problem. I also rapidly tried the k-nearest neighbours and \n",
    "Gaussian naive Bayes classifiers without a sizeable improvement in the results (not shown in the notebook). Overall the performance of all the three algorithms is similar. I guess that this is due to a good work in the feature engineering that \"leave less work\" to the algorithms. The classifier that seems to perform better is the SVC. <br>\n",
    "\n",
    "However, I would like to highlight a few concerns about the models in this notebook:\n",
    "- The models are trained using F1 as metric. While this is a reasonable choice the results in the subset accuracy can have sizeable fluctuations.\n",
    "- Training these models on subset accuracy (the full stability vector) is both technically difficult and conceptually not well defined. Indeed, now each compound with its own stoichiometry becomes to a certain extent an independent system.  \n",
    "- In a way this approach might loose the dependence between the labels in the same stability vector. For example, the stability of the (90% A-10% B) compound might have implication on the stability of (80% A-20% B).\n",
    "\n",
    "In conclusion, I believe that the models in this notebook are more suitable if the target is to find single stable compounds rather than predicting full stability vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
